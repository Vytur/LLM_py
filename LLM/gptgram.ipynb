{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "425de967-3d2a-4cdc-a82b-1283c2a467c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "from torch.nn import functional as F\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "block_size = 64\n",
    "batch_size = 128\n",
    "\n",
    "max_iters = 5000\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 100\n",
    "dropout = 0.2\n",
    "\n",
    "n_embd = 384 # vector that is 385 long about certain token\n",
    "n_head = 8\n",
    "n_layer = 8 # decoder count\n",
    "dropout = 0.2 # to prevent overfitting, dropping 20% of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5c220a0-d548-4aac-80da-9c430798d80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '#', '$', '%', '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'ç', 'é', 'ê', 'ô', '\\u200a', '—', '‘', '’', '“', '”', '•', '…', '™', '\\ufeff']\n"
     ]
    }
   ],
   "source": [
    "with open('The_great_gatsby.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "chars = sorted(set(text))\n",
    "vocab_size = len(chars)\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66d091a6-e140-4c98-958b-f34ed070b2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_int = {ch:i for i,ch in enumerate(chars)}\n",
    "int_to_string = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12196c8a-7cce-407b-a9b9-deb65c5915c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "tensor([[55, 58, 71,  ...,  1, 73, 68],\n",
      "        [58,  0, 54,  ..., 54, 73, 62],\n",
      "        [ 1, 30, 60,  ..., 72,  1, 74],\n",
      "        ...,\n",
      "        [78,  1, 62,  ..., 72,  1, 76],\n",
      "        [74, 60,  1,  ..., 67,  0, 58],\n",
      "        [58, 58, 67,  ..., 62, 64, 58]], device='cuda:0')\n",
      "targets:\n",
      "tensor([[58, 71, 60,  ..., 73, 68,  1],\n",
      "        [ 0, 54, 60,  ..., 73, 62, 68],\n",
      "        [30, 60, 60,  ...,  1, 74, 67],\n",
      "        ...,\n",
      "        [ 1, 62, 67,  ...,  1, 76, 54],\n",
      "        [60,  1, 55,  ...,  0, 58, 62],\n",
      "        [58, 67,  1,  ..., 64, 58,  1]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "n = int(0.8*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    get = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    #print(ix)\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch('train')\n",
    "print('inputs:')\n",
    "\n",
    "print(x)\n",
    "print('targets:')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "791f84d1-07e4-4e3f-8edb-43c87b24961a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when tensor([93]) target tensor(45)\n",
      "when tensor([93, 45]) target tensor(61)\n",
      "when tensor([93, 45, 61]) target tensor(58)\n",
      "when tensor([93, 45, 61, 58]) target tensor(1)\n",
      "when tensor([93, 45, 61, 58,  1]) target tensor(41)\n",
      "when tensor([93, 45, 61, 58,  1, 41]) target tensor(71)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71]) target tensor(68)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68]) target tensor(63)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63]) target tensor(58)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58]) target tensor(56)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56]) target tensor(73)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56, 73]) target tensor(1)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56, 73,  1]) target tensor(32)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56, 73,  1, 32]) target tensor(74)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56, 73,  1, 32, 74]) target tensor(73)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56, 73,  1, 32, 74, 73]) target tensor(58)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56, 73,  1, 32, 74, 73, 58]) target tensor(67)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56, 73,  1, 32, 74, 73, 58, 67]) target tensor(55)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56, 73,  1, 32, 74, 73, 58, 67,\n",
      "        55]) target tensor(58)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56, 73,  1, 32, 74, 73, 58, 67,\n",
      "        55, 58]) target tensor(71)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56, 73,  1, 32, 74, 73, 58, 67,\n",
      "        55, 58, 71]) target tensor(60)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56, 73,  1, 32, 74, 73, 58, 67,\n",
      "        55, 58, 71, 60]) target tensor(1)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56, 73,  1, 32, 74, 73, 58, 67,\n",
      "        55, 58, 71, 60,  1]) target tensor(58)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56, 73,  1, 32, 74, 73, 58, 67,\n",
      "        55, 58, 71, 60,  1, 58]) target tensor(27)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56, 73,  1, 32, 74, 73, 58, 67,\n",
      "        55, 58, 71, 60,  1, 58, 27]) target tensor(68)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56, 73,  1, 32, 74, 73, 58, 67,\n",
      "        55, 58, 71, 60,  1, 58, 27, 68]) target tensor(68)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56, 73,  1, 32, 74, 73, 58, 67,\n",
      "        55, 58, 71, 60,  1, 58, 27, 68, 68]) target tensor(64)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56, 73,  1, 32, 74, 73, 58, 67,\n",
      "        55, 58, 71, 60,  1, 58, 27, 68, 68, 64]) target tensor(1)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56, 73,  1, 32, 74, 73, 58, 67,\n",
      "        55, 58, 71, 60,  1, 58, 27, 68, 68, 64,  1]) target tensor(68)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56, 73,  1, 32, 74, 73, 58, 67,\n",
      "        55, 58, 71, 60,  1, 58, 27, 68, 68, 64,  1, 68]) target tensor(59)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56, 73,  1, 32, 74, 73, 58, 67,\n",
      "        55, 58, 71, 60,  1, 58, 27, 68, 68, 64,  1, 68, 59]) target tensor(1)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56, 73,  1, 32, 74, 73, 58, 67,\n",
      "        55, 58, 71, 60,  1, 58, 27, 68, 68, 64,  1, 68, 59,  1]) target tensor(45)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56, 73,  1, 32, 74, 73, 58, 67,\n",
      "        55, 58, 71, 60,  1, 58, 27, 68, 68, 64,  1, 68, 59,  1, 45]) target tensor(61)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56, 73,  1, 32, 74, 73, 58, 67,\n",
      "        55, 58, 71, 60,  1, 58, 27, 68, 68, 64,  1, 68, 59,  1, 45, 61]) target tensor(58)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56, 73,  1, 32, 74, 73, 58, 67,\n",
      "        55, 58, 71, 60,  1, 58, 27, 68, 68, 64,  1, 68, 59,  1, 45, 61, 58]) target tensor(1)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56, 73,  1, 32, 74, 73, 58, 67,\n",
      "        55, 58, 71, 60,  1, 58, 27, 68, 68, 64,  1, 68, 59,  1, 45, 61, 58,  1]) target tensor(32)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56, 73,  1, 32, 74, 73, 58, 67,\n",
      "        55, 58, 71, 60,  1, 58, 27, 68, 68, 64,  1, 68, 59,  1, 45, 61, 58,  1,\n",
      "        32]) target tensor(71)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56, 73,  1, 32, 74, 73, 58, 67,\n",
      "        55, 58, 71, 60,  1, 58, 27, 68, 68, 64,  1, 68, 59,  1, 45, 61, 58,  1,\n",
      "        32, 71]) target tensor(58)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56, 73,  1, 32, 74, 73, 58, 67,\n",
      "        55, 58, 71, 60,  1, 58, 27, 68, 68, 64,  1, 68, 59,  1, 45, 61, 58,  1,\n",
      "        32, 71, 58]) target tensor(54)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56, 73,  1, 32, 74, 73, 58, 67,\n",
      "        55, 58, 71, 60,  1, 58, 27, 68, 68, 64,  1, 68, 59,  1, 45, 61, 58,  1,\n",
      "        32, 71, 58, 54]) target tensor(73)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56, 73,  1, 32, 74, 73, 58, 67,\n",
      "        55, 58, 71, 60,  1, 58, 27, 68, 68, 64,  1, 68, 59,  1, 45, 61, 58,  1,\n",
      "        32, 71, 58, 54, 73]) target tensor(1)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56, 73,  1, 32, 74, 73, 58, 67,\n",
      "        55, 58, 71, 60,  1, 58, 27, 68, 68, 64,  1, 68, 59,  1, 45, 61, 58,  1,\n",
      "        32, 71, 58, 54, 73,  1]) target tensor(32)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56, 73,  1, 32, 74, 73, 58, 67,\n",
      "        55, 58, 71, 60,  1, 58, 27, 68, 68, 64,  1, 68, 59,  1, 45, 61, 58,  1,\n",
      "        32, 71, 58, 54, 73,  1, 32]) target tensor(54)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56, 73,  1, 32, 74, 73, 58, 67,\n",
      "        55, 58, 71, 60,  1, 58, 27, 68, 68, 64,  1, 68, 59,  1, 45, 61, 58,  1,\n",
      "        32, 71, 58, 54, 73,  1, 32, 54]) target tensor(73)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56, 73,  1, 32, 74, 73, 58, 67,\n",
      "        55, 58, 71, 60,  1, 58, 27, 68, 68, 64,  1, 68, 59,  1, 45, 61, 58,  1,\n",
      "        32, 71, 58, 54, 73,  1, 32, 54, 73]) target tensor(72)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56, 73,  1, 32, 74, 73, 58, 67,\n",
      "        55, 58, 71, 60,  1, 58, 27, 68, 68, 64,  1, 68, 59,  1, 45, 61, 58,  1,\n",
      "        32, 71, 58, 54, 73,  1, 32, 54, 73, 72]) target tensor(55)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56, 73,  1, 32, 74, 73, 58, 67,\n",
      "        55, 58, 71, 60,  1, 58, 27, 68, 68, 64,  1, 68, 59,  1, 45, 61, 58,  1,\n",
      "        32, 71, 58, 54, 73,  1, 32, 54, 73, 72, 55]) target tensor(78)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56, 73,  1, 32, 74, 73, 58, 67,\n",
      "        55, 58, 71, 60,  1, 58, 27, 68, 68, 64,  1, 68, 59,  1, 45, 61, 58,  1,\n",
      "        32, 71, 58, 54, 73,  1, 32, 54, 73, 72, 55, 78]) target tensor(0)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56, 73,  1, 32, 74, 73, 58, 67,\n",
      "        55, 58, 71, 60,  1, 58, 27, 68, 68, 64,  1, 68, 59,  1, 45, 61, 58,  1,\n",
      "        32, 71, 58, 54, 73,  1, 32, 54, 73, 72, 55, 78,  0]) target tensor(1)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56, 73,  1, 32, 74, 73, 58, 67,\n",
      "        55, 58, 71, 60,  1, 58, 27, 68, 68, 64,  1, 68, 59,  1, 45, 61, 58,  1,\n",
      "        32, 71, 58, 54, 73,  1, 32, 54, 73, 72, 55, 78,  0,  1]) target tensor(1)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56, 73,  1, 32, 74, 73, 58, 67,\n",
      "        55, 58, 71, 60,  1, 58, 27, 68, 68, 64,  1, 68, 59,  1, 45, 61, 58,  1,\n",
      "        32, 71, 58, 54, 73,  1, 32, 54, 73, 72, 55, 78,  0,  1,  1]) target tensor(1)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56, 73,  1, 32, 74, 73, 58, 67,\n",
      "        55, 58, 71, 60,  1, 58, 27, 68, 68, 64,  1, 68, 59,  1, 45, 61, 58,  1,\n",
      "        32, 71, 58, 54, 73,  1, 32, 54, 73, 72, 55, 78,  0,  1,  1,  1]) target tensor(1)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56, 73,  1, 32, 74, 73, 58, 67,\n",
      "        55, 58, 71, 60,  1, 58, 27, 68, 68, 64,  1, 68, 59,  1, 45, 61, 58,  1,\n",
      "        32, 71, 58, 54, 73,  1, 32, 54, 73, 72, 55, 78,  0,  1,  1,  1,  1]) target tensor(0)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56, 73,  1, 32, 74, 73, 58, 67,\n",
      "        55, 58, 71, 60,  1, 58, 27, 68, 68, 64,  1, 68, 59,  1, 45, 61, 58,  1,\n",
      "        32, 71, 58, 54, 73,  1, 32, 54, 73, 72, 55, 78,  0,  1,  1,  1,  1,  0]) target tensor(45)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56, 73,  1, 32, 74, 73, 58, 67,\n",
      "        55, 58, 71, 60,  1, 58, 27, 68, 68, 64,  1, 68, 59,  1, 45, 61, 58,  1,\n",
      "        32, 71, 58, 54, 73,  1, 32, 54, 73, 72, 55, 78,  0,  1,  1,  1,  1,  0,\n",
      "        45]) target tensor(61)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56, 73,  1, 32, 74, 73, 58, 67,\n",
      "        55, 58, 71, 60,  1, 58, 27, 68, 68, 64,  1, 68, 59,  1, 45, 61, 58,  1,\n",
      "        32, 71, 58, 54, 73,  1, 32, 54, 73, 72, 55, 78,  0,  1,  1,  1,  1,  0,\n",
      "        45, 61]) target tensor(62)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56, 73,  1, 32, 74, 73, 58, 67,\n",
      "        55, 58, 71, 60,  1, 58, 27, 68, 68, 64,  1, 68, 59,  1, 45, 61, 58,  1,\n",
      "        32, 71, 58, 54, 73,  1, 32, 54, 73, 72, 55, 78,  0,  1,  1,  1,  1,  0,\n",
      "        45, 61, 62]) target tensor(72)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56, 73,  1, 32, 74, 73, 58, 67,\n",
      "        55, 58, 71, 60,  1, 58, 27, 68, 68, 64,  1, 68, 59,  1, 45, 61, 58,  1,\n",
      "        32, 71, 58, 54, 73,  1, 32, 54, 73, 72, 55, 78,  0,  1,  1,  1,  1,  0,\n",
      "        45, 61, 62, 72]) target tensor(1)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56, 73,  1, 32, 74, 73, 58, 67,\n",
      "        55, 58, 71, 60,  1, 58, 27, 68, 68, 64,  1, 68, 59,  1, 45, 61, 58,  1,\n",
      "        32, 71, 58, 54, 73,  1, 32, 54, 73, 72, 55, 78,  0,  1,  1,  1,  1,  0,\n",
      "        45, 61, 62, 72,  1]) target tensor(58)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56, 73,  1, 32, 74, 73, 58, 67,\n",
      "        55, 58, 71, 60,  1, 58, 27, 68, 68, 64,  1, 68, 59,  1, 45, 61, 58,  1,\n",
      "        32, 71, 58, 54, 73,  1, 32, 54, 73, 72, 55, 78,  0,  1,  1,  1,  1,  0,\n",
      "        45, 61, 62, 72,  1, 58]) target tensor(55)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56, 73,  1, 32, 74, 73, 58, 67,\n",
      "        55, 58, 71, 60,  1, 58, 27, 68, 68, 64,  1, 68, 59,  1, 45, 61, 58,  1,\n",
      "        32, 71, 58, 54, 73,  1, 32, 54, 73, 72, 55, 78,  0,  1,  1,  1,  1,  0,\n",
      "        45, 61, 62, 72,  1, 58, 55]) target tensor(68)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56, 73,  1, 32, 74, 73, 58, 67,\n",
      "        55, 58, 71, 60,  1, 58, 27, 68, 68, 64,  1, 68, 59,  1, 45, 61, 58,  1,\n",
      "        32, 71, 58, 54, 73,  1, 32, 54, 73, 72, 55, 78,  0,  1,  1,  1,  1,  0,\n",
      "        45, 61, 62, 72,  1, 58, 55, 68]) target tensor(68)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56, 73,  1, 32, 74, 73, 58, 67,\n",
      "        55, 58, 71, 60,  1, 58, 27, 68, 68, 64,  1, 68, 59,  1, 45, 61, 58,  1,\n",
      "        32, 71, 58, 54, 73,  1, 32, 54, 73, 72, 55, 78,  0,  1,  1,  1,  1,  0,\n",
      "        45, 61, 62, 72,  1, 58, 55, 68, 68]) target tensor(64)\n",
      "when tensor([93, 45, 61, 58,  1, 41, 71, 68, 63, 58, 56, 73,  1, 32, 74, 73, 58, 67,\n",
      "        55, 58, 71, 60,  1, 58, 27, 68, 68, 64,  1, 68, 59,  1, 45, 61, 58,  1,\n",
      "        32, 71, 58, 54, 73,  1, 32, 54, 73, 72, 55, 78,  0,  1,  1,  1,  1,  0,\n",
      "        45, 61, 62, 72,  1, 58, 55, 68, 68, 64]) target tensor(1)\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for tr in range(block_size):\n",
    "    context = x[:tr+1]\n",
    "    target = y[tr]\n",
    "    print('when', context, 'target', target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36bf733a-1a78-4338-bed6-2ba43239d375",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() # good for reporting\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval() # model evaluation, dropout disabled\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # model training mode, dropout enabled\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "803c239f-8c9f-4785-9de8-79d53fab974b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise neural network\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head = n_head) for _ in range(n_layer)]) # for encoder layers\n",
    "\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean = 0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module,nn.Embedding):\n",
    "                torch.nn.init.normal_(module.weight, mean = 0.0, std=0.02)\n",
    "\n",
    "    def forward(self, index, targets=None):\n",
    "        # logits = self.token_embedding_table(index)\n",
    "        B, T = index.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(index) # (B, T, C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
    "        x = tok_emb + pos_emb # (B, T, C)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape # B batch, T time, C channels (vocab size)\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, index, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss= self.forward(index) # getting predictions\n",
    "            logits = logits[:, -1, :] # becomes (B, C), focis only on last time step\n",
    "            probs = F.softmax(logits, dim=-1) # softmax function to get probabilities\n",
    "            index_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            index = torch.cat((index, index_next), dim=1) # (B, T+1), append sampled index to running sequence\n",
    "        return index\n",
    "\n",
    "class Block(nn.Module):\n",
    "    # communication followed by computation\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd embedding dimensions\n",
    "        # n_head number of heads\n",
    "        super().__init__() \n",
    "        head_size = n_embd // n_head # division\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.sa(x) # self attention\n",
    "        x = self.ln1(x+y) # add a layer norm\n",
    "        y = self.ffwd(x) # feed forward\n",
    "        x = self.ln2(x+y) # add a layer norm\n",
    "        return x\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    # linear layer followed by a non-linearity\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    # multiple heads of self-attention in parallel\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)]) # runs heads in parallel\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd) # projection to be hackable?, also can add , bias=False\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # concatinate each head together along the last dimention (B, T, F(this one))\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    # head of self-attention\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False) # transoform head size\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x) # (B, T, hs)\n",
    "        q = self.query(x)\n",
    "\n",
    "        # conpute attention scores\n",
    "        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        # weighted aggregation of the values\n",
    "        v = self.value(x) # (B, T, hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = GPTLanguageModel(vocab_size)\n",
    "\n",
    "print('loading model parameters.')\n",
    "with open('model_01.pkl', 'wb') as f\n",
    "    model = pickle.load(f)\n",
    "print('loaded.')\n",
    "\n",
    "m = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22b2d324-e001-4486-8004-c9272d58183d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train loss: 4.6487836837768555, val loss: 4.647613048553467\n",
      "step: 100, train loss: 2.342151403427124, val loss: 2.340352773666382\n",
      "step: 200, train loss: 2.037874221801758, val loss: 2.0387651920318604\n",
      "step: 300, train loss: 1.862667202949524, val loss: 1.8642381429672241\n",
      "step: 400, train loss: 1.7432917356491089, val loss: 1.7405171394348145\n",
      "step: 500, train loss: 1.6528804302215576, val loss: 1.658371090888977\n",
      "step: 600, train loss: 1.5829946994781494, val loss: 1.5836008787155151\n",
      "step: 700, train loss: 1.5250871181488037, val loss: 1.527219533920288\n",
      "step: 800, train loss: 1.4830257892608643, val loss: 1.4844839572906494\n",
      "step: 900, train loss: 1.4421733617782593, val loss: 1.438604712486267\n",
      "step: 1000, train loss: 1.4036064147949219, val loss: 1.402845859527588\n",
      "step: 1100, train loss: 1.3683358430862427, val loss: 1.3646725416183472\n",
      "step: 1200, train loss: 1.3418959379196167, val loss: 1.3440251350402832\n",
      "step: 1300, train loss: 1.3124696016311646, val loss: 1.3121479749679565\n",
      "step: 1400, train loss: 1.2950303554534912, val loss: 1.2944920063018799\n",
      "step: 1500, train loss: 1.266760230064392, val loss: 1.2660174369812012\n",
      "step: 1600, train loss: 1.2478185892105103, val loss: 1.2452269792556763\n",
      "step: 1700, train loss: 1.2239768505096436, val loss: 1.2250914573669434\n",
      "step: 1800, train loss: 1.2089109420776367, val loss: 1.2085652351379395\n",
      "step: 1900, train loss: 1.185975193977356, val loss: 1.1871131658554077\n",
      "step: 2000, train loss: 1.169865369796753, val loss: 1.168698787689209\n",
      "step: 2100, train loss: 1.1516860723495483, val loss: 1.147386908531189\n",
      "step: 2200, train loss: 1.1325886249542236, val loss: 1.1308175325393677\n",
      "step: 2300, train loss: 1.1219607591629028, val loss: 1.1225749254226685\n",
      "step: 2400, train loss: 1.1046394109725952, val loss: 1.1047378778457642\n",
      "step: 2500, train loss: 1.0898500680923462, val loss: 1.0883164405822754\n",
      "step: 2600, train loss: 1.0731607675552368, val loss: 1.0752589702606201\n",
      "step: 2700, train loss: 1.0596812963485718, val loss: 1.0579811334609985\n",
      "step: 2800, train loss: 1.0384942293167114, val loss: 1.0386250019073486\n",
      "step: 2900, train loss: 1.0274910926818848, val loss: 1.026037573814392\n",
      "step: 3000, train loss: 1.012429118156433, val loss: 1.013921856880188\n",
      "step: 3100, train loss: 1.0006184577941895, val loss: 1.001428484916687\n",
      "step: 3200, train loss: 0.9845636487007141, val loss: 0.9840112924575806\n",
      "step: 3300, train loss: 0.9649965167045593, val loss: 0.9681663513183594\n",
      "step: 3400, train loss: 0.9581266641616821, val loss: 0.9532087445259094\n",
      "step: 3500, train loss: 0.941100001335144, val loss: 0.943398118019104\n",
      "step: 3600, train loss: 0.9308123588562012, val loss: 0.9305307865142822\n",
      "step: 3700, train loss: 0.9202360510826111, val loss: 0.9150669574737549\n",
      "step: 3800, train loss: 0.9055798053741455, val loss: 0.9034078121185303\n",
      "step: 3900, train loss: 0.8860216736793518, val loss: 0.8890237212181091\n",
      "step: 4000, train loss: 0.8781879544258118, val loss: 0.8766645789146423\n",
      "step: 4100, train loss: 0.863649308681488, val loss: 0.8597486615180969\n",
      "step: 4200, train loss: 0.854844868183136, val loss: 0.8548418283462524\n",
      "step: 4300, train loss: 0.8395360708236694, val loss: 0.8405971527099609\n",
      "step: 4400, train loss: 0.8274397253990173, val loss: 0.8269422650337219\n",
      "step: 4500, train loss: 0.8119722008705139, val loss: 0.8123253583908081\n",
      "step: 4600, train loss: 0.8012518882751465, val loss: 0.8009899854660034\n",
      "step: 4700, train loss: 0.7929287552833557, val loss: 0.7943745255470276\n",
      "step: 4800, train loss: 0.7840136885643005, val loss: 0.7818639874458313\n",
      "step: 4900, train loss: 0.7703622579574585, val loss: 0.769915759563446\n"
     ]
    }
   ],
   "source": [
    "#optimizes\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step: {iter}, train loss: {losses['train']}, val loss: {losses['val']}\")\n",
    "    \n",
    "    xb, yb = get_batch('train') # sample batch of data\n",
    "    logits, loss = model.forward(xb, yb) #evaluate the loss\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    #print(loss.item())\n",
    "\n",
    "with open('model_01.pkl', 'wb') as f\n",
    "    pickle.dump(model, f)\n",
    "print('model saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38c8fb48-d7e4-4a06-b336-95cfa09df693",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (65) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m context \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m----> 2\u001b[0m generated_chars \u001b[38;5;241m=\u001b[39m decode(\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(generated_chars)\n",
      "Cell \u001b[1;32mIn[7], line 45\u001b[0m, in \u001b[0;36mGPTLanguageModel.generate\u001b[1;34m(self, index, max_new_tokens)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m, index, max_new_tokens):\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_new_tokens):\n\u001b[1;32m---> 45\u001b[0m         logits, loss\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# getting predictions\u001b[39;00m\n\u001b[0;32m     46\u001b[0m         logits \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :] \u001b[38;5;66;03m# becomes (B, C), focis only on last time step\u001b[39;00m\n\u001b[0;32m     47\u001b[0m         probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# softmax function to get probabilities\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 29\u001b[0m, in \u001b[0;36mGPTLanguageModel.forward\u001b[1;34m(self, index, targets)\u001b[0m\n\u001b[0;32m     27\u001b[0m pos_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_table(torch\u001b[38;5;241m.\u001b[39marange(T, device\u001b[38;5;241m=\u001b[39mdevice)) \u001b[38;5;66;03m# (T, C)\u001b[39;00m\n\u001b[0;32m     28\u001b[0m x \u001b[38;5;241m=\u001b[39m tok_emb \u001b[38;5;241m+\u001b[39m pos_emb \u001b[38;5;66;03m# (B, T, C)\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_f(x)\n\u001b[0;32m     31\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(x) \u001b[38;5;66;03m# (B, T, vocab_size)\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python\\cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python\\cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python\\cuda\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mC:\\Python\\cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python\\cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 65\u001b[0m, in \u001b[0;36mBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 65\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msa\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# self attention\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(x\u001b[38;5;241m+\u001b[39my) \u001b[38;5;66;03m# add a layer norm\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffwd(x) \u001b[38;5;66;03m# feed forward\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python\\cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python\\cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 93\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 93\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\u001b[43m[\u001b[49m\u001b[43mh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheads\u001b[49m\u001b[43m]\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# concatinate each head together along the last dimention (B, T, F(this one))\u001b[39;00m\n\u001b[0;32m     94\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(out))\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "Cell \u001b[1;32mIn[7], line 93\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 93\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[43mh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# concatinate each head together along the last dimention (B, T, F(this one))\u001b[39;00m\n\u001b[0;32m     94\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(out))\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mC:\\Python\\cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python\\cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 116\u001b[0m, in \u001b[0;36mHead.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# conpute attention scores\u001b[39;00m\n\u001b[0;32m    115\u001b[0m wei \u001b[38;5;241m=\u001b[39m q \u001b[38;5;241m@\u001b[39m k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m k\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;66;03m# (B, T, hs) @ (B, hs, T) -> (B, T, T)\u001b[39;00m\n\u001b[1;32m--> 116\u001b[0m wei \u001b[38;5;241m=\u001b[39m \u001b[43mwei\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_fill\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtril\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mT\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-inf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B, T, T)\u001b[39;00m\n\u001b[0;32m    117\u001b[0m wei \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(wei, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    118\u001b[0m wei \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(wei)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (65) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens = 500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16f0bea-6390-4408-9158-df4a53b386f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_t",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
